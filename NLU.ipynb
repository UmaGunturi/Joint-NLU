{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKeTCvVfOgXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_gpu==2.0.0-alpha0\n",
        "\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6MWvRuw5qBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow_hub\n",
        "#import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q_AmIPpZUNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfBoYytHQGgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip src\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKH1Rn9IQggx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/src')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRvJ2KpDRKHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "#from tensorflow.python.ops import rnn_cell_impl\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from utils import createVocabulary\n",
        "from utils import loadVocabulary\n",
        "from utils import computeF1Score\n",
        "from utils import DataProcessor\n",
        "\n",
        "parser = argparse.ArgumentParser(allow_abbrev=False)\n",
        "\n",
        "#Network\n",
        "parser.add_argument(\"--num_units\", type=int, default=128, help=\"Network size.\", dest='layer_size')\n",
        "parser.add_argument(\"--model_type\", type=str, default='full', help=\"\"\"full(default) | intent_only\n",
        "                                                                    full: full attention model\n",
        "                                                                    intent_only: intent attention model\"\"\")\n",
        "\n",
        "#Training Environment\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=20, help=\"Max epochs to train.\")\n",
        "parser.add_argument(\"--no_early_stop\", action='store_false',dest='early_stop', help=\"Disable early stop, which is based on sentence level accuracy.\")\n",
        "parser.add_argument(\"--patience\", type=int, default=5, help=\"Patience to wait before stop.\")\n",
        "\n",
        "#Model and Vocab\n",
        "parser.add_argument(\"--dataset\", type=str, default=None, help=\"\"\"Type 'atis' or 'snips' to use dataset provided by us or enter what ever you named your own dataset.\n",
        "                Note, if you don't want to use this part, enter --dataset=''. It can not be None\"\"\")\n",
        "parser.add_argument(\"--model_path\", type=str, default='./src/model', help=\"Path to save model.\")\n",
        "parser.add_argument(\"--vocab_path\", type=str, default='./src/vocab', help=\"Path to vocabulary files.\")\n",
        "\n",
        "#Data\n",
        "parser.add_argument(\"--train_data_path\", type=str, default='train', help=\"Path to training data files.\")\n",
        "parser.add_argument(\"--test_data_path\", type=str, default='test', help=\"Path to testing data files.\")\n",
        "parser.add_argument(\"--valid_data_path\", type=str, default='valid', help=\"Path to validation data files.\")\n",
        "parser.add_argument(\"--input_file\", type=str, default='seq.in', help=\"Input file name.\")\n",
        "parser.add_argument(\"--slot_file\", type=str, default='seq.out', help=\"Slot file name.\")\n",
        "parser.add_argument(\"--intent_file\", type=str, default='label', help=\"Intent file name.\")\n",
        "\n",
        "arg=parser.parse_args([\"--dataset\",\"atis\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJw4ZYsyRpJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Print arguments\n",
        "for k,v in sorted(vars(arg).items()):\n",
        "    print(k,'=',v)\n",
        "print()\n",
        "\n",
        "if arg.model_type == 'full':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = False\n",
        "elif arg.model_type == 'intent_only':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = True\n",
        "else:\n",
        "    print('unknown model type!')\n",
        "    exit(1)\n",
        "\n",
        "#full path to data will be: ./data + dataset + train/test/valid\n",
        "if arg.dataset == None:\n",
        "    print('name of dataset can not be None')\n",
        "    exit(1)\n",
        "elif arg.dataset == 'snips':\n",
        "    print('use snips dataset')\n",
        "elif arg.dataset == 'atis':\n",
        "    print('use atis dataset')\n",
        "else:\n",
        "    print('use own dataset: ',arg.dataset)\n",
        "full_train_path = os.path.join('./src/data',arg.dataset,arg.train_data_path)\n",
        "full_test_path = os.path.join('./src/data',arg.dataset,arg.test_data_path)\n",
        "full_valid_path = os.path.join('./src/data',arg.dataset,arg.valid_data_path)\n",
        "\n",
        "createVocabulary(os.path.join(full_train_path, arg.input_file), os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "createVocabulary(os.path.join(full_train_path, arg.slot_file), os.path.join(arg.vocab_path, 'slot_vocab'))\n",
        "createVocabulary(os.path.join(full_train_path, arg.intent_file), os.path.join(arg.vocab_path, 'intent_vocab'))\n",
        "\n",
        "in_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "slot_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'slot_vocab'))\n",
        "intent_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'intent_vocab'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxCeThXUK1bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NOT USING CURRENTLY\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22Bbh5PQWFbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "class CustomDropout(layers.Layer):\n",
        "  def __init__(self, rate, **kwargs):\n",
        "    super(CustomDropout, self).__init__(**kwargs)\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "        return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs\n",
        "  \n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, input_size, slot_size, intent_size, layer_size = 128):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(input_size, layer_size)\n",
        "    self.bilstm = tf.keras.layers.Bidirectional(CuDNNLSTM(layer_size, return_sequences=True,return_state=True))\n",
        "    self.dropout = CustomDropout(0.5)\n",
        "    self.intent_out = tf.keras.layers.Dense(intent_size, activation=None)\n",
        "    self.slot_out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(slot_size, activation=None))\n",
        "  \n",
        "  @tf.function  \n",
        "  def call(self, inputs, sequence_length, isTraining=True):\n",
        "    x = self.embedding(inputs)\n",
        "    state_outputs, forward_h, forward_c, backward_h, backward_c = self.bilstm(x)\n",
        "    \n",
        "    state_outputs = self.dropout(state_outputs, isTraining)\n",
        "    forward_h = self.dropout(forward_h, isTraining)\n",
        "    backward_h = self.dropout(backward_h, isTraining)\n",
        "   \n",
        "    final_state = tf.keras.layers.concatenate([forward_h,backward_h])\n",
        "    intent = self.intent_out(final_state)\n",
        "    slots = self.slot_out(state_outputs)\n",
        "    outputs = [slots, intent]\n",
        "    return outputs\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4WTGWQX4AH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = MyModel(len(in_vocab['vocab']), len(slot_vocab['vocab']), len(intent_vocab['vocab']), layer_size=arg.layer_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOx6foM60O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"import time\n",
        "t = time.perf_counter() #time.process_time()\n",
        "slots, intent = model(in_data, length)\n",
        "elapsed = time.perf_counter() - t\n",
        "print(elapsed)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geNgBiAPXffN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def valid(in_path, slot_path, intent_path):\n",
        "    data_processor_valid = DataProcessor(in_path, slot_path, intent_path, in_vocab, slot_vocab, intent_vocab)\n",
        "    pred_intents = []\n",
        "    correct_intents = []\n",
        "    slot_outputs = []\n",
        "    correct_slots = []\n",
        "    input_words = []\n",
        "\n",
        "    #used to gate\n",
        "    #gate_seq = []\n",
        "    while True:\n",
        "        in_data, slot_data, slot_weight, length, intents, in_seq, slot_seq, intent_seq = data_processor_valid.get_batch(arg.batch_size)\n",
        "        #feed_dict = {input_data.name: in_data, sequence_length.name: length}\n",
        "        #ret = sess.run(inference_outputs, feed_dict)\n",
        "        slots, intent = model(in_data, length, isTraining = False)\n",
        "        for i in np.array(intent):\n",
        "            pred_intents.append(np.argmax(i))\n",
        "        for i in intents:\n",
        "            correct_intents.append(i)\n",
        "\n",
        "        pred_slots = slots\n",
        "        for p, t, i, l in zip(pred_slots, slot_data, in_data, length):\n",
        "            p = np.argmax(p, 1)\n",
        "            tmp_pred = []\n",
        "            tmp_correct = []\n",
        "            tmp_input = []\n",
        "            for j in range(l):\n",
        "                tmp_pred.append(slot_vocab['rev'][p[j]])\n",
        "                tmp_correct.append(slot_vocab['rev'][t[j]])\n",
        "                tmp_input.append(in_vocab['rev'][i[j]])\n",
        "\n",
        "            slot_outputs.append(tmp_pred)\n",
        "            correct_slots.append(tmp_correct)\n",
        "            input_words.append(tmp_input)\n",
        "\n",
        "        if data_processor_valid.end == 1:\n",
        "            break\n",
        "\n",
        "    pred_intents = np.array(pred_intents)\n",
        "    correct_intents = np.array(correct_intents)\n",
        "    accuracy = (pred_intents==correct_intents)\n",
        "    semantic_error = accuracy\n",
        "    accuracy = accuracy.astype(float)\n",
        "    accuracy = np.mean(accuracy)*100.0\n",
        "\n",
        "    index = 0\n",
        "    for t, p in zip(correct_slots, slot_outputs):\n",
        "        # Process Semantic Error\n",
        "        if len(t) != len(p):\n",
        "            raise ValueError('Error!!')\n",
        "\n",
        "        for j in range(len(t)):\n",
        "            if p[j] != t[j]:\n",
        "                semantic_error[index] = False\n",
        "                break\n",
        "        index += 1\n",
        "    semantic_error = semantic_error.astype(float)\n",
        "    semantic_error = np.mean(semantic_error)*100.0\n",
        "\n",
        "    f1, precision, recall = computeF1Score(correct_slots, slot_outputs)\n",
        "    print('slot f1: ' + str(f1) + '\\tintent accuracy: ' + str(accuracy) + '\\tsemantic_error: ' + str(semantic_error))\n",
        "    #print('intent accuracy: ' + str(accuracy))\n",
        "    #print('semantic error(intent, slots are all correct): ' + str(semantic_error))\n",
        "\n",
        "    data_processor_valid.close()\n",
        "    return f1,accuracy,semantic_error,pred_intents,correct_intents,slot_outputs,correct_slots,input_words#,gate_seq\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrfg5-eOY9Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(-1), optimizer=opt, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, arg.model_path + \"/\" + arg.dataset + '/tf_ckpts', max_to_keep=3)\n",
        "data_processor = None\n",
        "valid_err = 0\n",
        "save_path = os.path.join(arg.model_path , str(arg.dataset) + \"/\")\n",
        "for epoch in range(50):\n",
        "    while True:\n",
        "        if data_processor == None:\n",
        "            i_loss = 0\n",
        "            s_loss = 0\n",
        "            batches = 0\n",
        "            data_processor = DataProcessor(os.path.join(full_train_path, arg.input_file), os.path.join(full_train_path, arg.slot_file), os.path.join(full_train_path, arg.intent_file), in_vocab, slot_vocab, intent_vocab)\n",
        "        in_data, slot_labels, slot_weights, length, intent_labels,in_seq,_,_ = data_processor.get_batch(arg.batch_size)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "          slots, intent = model(in_data, length, isTraining = True)\n",
        "          intent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=intent_labels, logits=intent)  \n",
        "          #slot_loss\n",
        "          slots_out = tf.reshape(slots, [-1,len(slot_vocab['vocab'])])\n",
        "          slots_shape = tf.shape(slot_labels)\n",
        "          slot_reshape = tf.reshape(slot_labels, [-1])\n",
        "          crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=slot_reshape, logits=slots_out)\n",
        "          crossent = tf.reshape(crossent, slots_shape)\n",
        "          slot_loss = tf.reduce_sum(crossent*slot_weights, 1)\n",
        "          total_size = tf.reduce_sum(slot_weights, 1)\n",
        "          total_size += 1e-12\n",
        "          slot_loss = slot_loss / total_size\n",
        "          \n",
        "          total_loss = intent_loss + slot_loss\n",
        "        \n",
        "        grads = tape.gradient(total_loss, model.trainable_weights)\n",
        "        opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        s_loss = s_loss + tf.reduce_sum(slot_loss)/tf.cast(arg.batch_size, tf.float32)\n",
        "        i_loss = i_loss +  tf.reduce_sum(intent_loss)/tf.cast(arg.batch_size, tf.float32)\n",
        "        batches = batches + 1\n",
        "        if data_processor.end == 1:\n",
        "            data_processor.close()\n",
        "            data_processor = None\n",
        "            break\n",
        "            \n",
        "    #print(\"Training Epoch: \" ,epoch,\" Slot Loss: \",s_loss/batches, \" Intent_Loss: \", i_loss/batches)\n",
        "    print(\"EPOCH: \", epoch, \" *******************************************************************\")\n",
        "    print('Train:', end=\"\\t\")\n",
        "    _ = valid(os.path.join(full_train_path, arg.input_file), os.path.join(full_train_path, arg.slot_file), os.path.join(full_train_path, arg.intent_file))\n",
        "    \n",
        "    print('Valid:', end=\"\\t\")\n",
        "    epoch_valid_slot, epoch_valid_intent, epoch_valid_err,valid_pred_intent,valid_correct_intent,valid_pred_slot,valid_correct_slot,valid_words = valid(os.path.join(full_valid_path, arg.input_file), os.path.join(full_valid_path, arg.slot_file), os.path.join(full_valid_path, arg.intent_file))\n",
        "\n",
        "    print('Test:', end=\"\\t\")\n",
        "    epoch_test_slot, epoch_test_intent, epoch_test_err,test_pred_intent,test_correct_intent,test_pred_slot,test_correct_slot,test_words = valid(os.path.join(full_test_path, arg.input_file), os.path.join(full_test_path, arg.slot_file), os.path.join(full_test_path, arg.intent_file))\n",
        "    \n",
        "    ckpt.step.assign_add(1)\n",
        "    if epoch_valid_err <= valid_err:\n",
        "        no_improve += 1\n",
        "    else:\n",
        "        valid_err = epoch_valid_err\n",
        "        no_improve = 0\n",
        "        print(\"Saving\", str(ckpt.step), \"with valid accuracy:\", valid_err   )\n",
        "        save_path = manager.save()\n",
        "\n",
        "    if arg.early_stop == True:\n",
        "        if no_improve > arg.patience:\n",
        "            print(\"EARLY BREAK\")\n",
        "            break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o15eK9npnOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}