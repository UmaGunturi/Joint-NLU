{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_SlotGated.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SKeTCvVfOgXu",
        "outputId": "bb849cb9-4503-4bb7-f070-280561263da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "!pip install tensorflow_gpu==2.0.0-alpha0\n",
        "##!pip install tensorflow_hub"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 62kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (1.16.4)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow_gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (1.12.0)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow_gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (1.15.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow_gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow_gpu==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow_gpu==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow_gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_gpu==2.0.0-alpha0) (41.0.1)\n",
            "Installing collected packages: tb-nightly, google-pasta, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.7 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OfBoYytHQGgs",
        "outputId": "27ed975a-92d0-4349-fa04-82d7757668dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "!unzip src"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  src.zip\n",
            "   creating: src/.ipynb_checkpoints/\n",
            "  inflating: src/.ipynb_checkpoints/utils-checkpoint.py  \n",
            "   creating: src/__pycache__/\n",
            "  inflating: src/__pycache__/utils.cpython-36.pyc  \n",
            "   creating: src/data/\n",
            "   creating: src/data/atis/\n",
            "   creating: src/data/atis/test/\n",
            "  inflating: src/data/atis/test/label  \n",
            "  inflating: src/data/atis/test/seq.in  \n",
            "  inflating: src/data/atis/test/seq.out  \n",
            "   creating: src/data/atis/train/\n",
            "  inflating: src/data/atis/train/label  \n",
            "  inflating: src/data/atis/train/seq.in  \n",
            "  inflating: src/data/atis/train/seq.out  \n",
            "   creating: src/data/atis/valid/\n",
            "  inflating: src/data/atis/valid/label  \n",
            "  inflating: src/data/atis/valid/seq.in  \n",
            "  inflating: src/data/atis/valid/seq.out  \n",
            "   creating: src/data/snips/\n",
            "   creating: src/data/snips/test/\n",
            "  inflating: src/data/snips/test/label  \n",
            "  inflating: src/data/snips/test/seq.in  \n",
            "  inflating: src/data/snips/test/seq.out  \n",
            "   creating: src/data/snips/train/\n",
            "  inflating: src/data/snips/train/label  \n",
            "  inflating: src/data/snips/train/seq.in  \n",
            "  inflating: src/data/snips/train/seq.out  \n",
            "   creating: src/data/snips/valid/\n",
            "  inflating: src/data/snips/valid/label  \n",
            "  inflating: src/data/snips/valid/seq.in  \n",
            "  inflating: src/data/snips/valid/seq.out  \n",
            "   creating: src/model/\n",
            " extracting: src/model/.gitignore    \n",
            "  inflating: src/utils.py            \n",
            "   creating: src/vocab/\n",
            " extracting: src/vocab/in_vocab      \n",
            "  inflating: src/vocab/intent_vocab  \n",
            "  inflating: src/vocab/slot_vocab    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oKH1Rn9IQggx",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/src')\n",
        "#sys.path.insert(0, './src')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5wTYGqhHALvz",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Layer, Conv1D,TimeDistributed, Bidirectional, Dense, Embedding\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, LSTM\n",
        "#import tensorflow_hub as hub\n",
        "from utils import createVocabulary\n",
        "from utils import loadVocabulary\n",
        "from utils import computeF1Score\n",
        "from utils import DataProcessor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nRvJ2KpDRKHw",
        "colab": {}
      },
      "source": [
        "\n",
        "parser = argparse.ArgumentParser(allow_abbrev=False)\n",
        "\n",
        "#Network\n",
        "parser.add_argument(\"--num_units\", type=int, default=64, help=\"Network size.\", dest='layer_size')\n",
        "parser.add_argument(\"--model_type\", type=str, default='full', help=\"\"\"full(default) | intent_only\n",
        "                                                                    full: full attention model\n",
        "                                                                    intent_only: intent attention model\"\"\")\n",
        "\n",
        "#Training Environment\n",
        "parser.add_argument(\"--batch_size\", type=int, default=48, help=\"Batch size.\")\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=50, help=\"Max epochs to train.\")\n",
        "parser.add_argument(\"--no_early_stop\", action='store_false',dest='early_stop', help=\"Disable early stop, which is based on sentence level accuracy.\")\n",
        "parser.add_argument(\"--patience\", type=int, default=20, help=\"Patience to wait before stop.\")\n",
        "\n",
        "#Model and Vocab\n",
        "parser.add_argument(\"--dataset\", type=str, default=None, help=\"\"\"Type 'atis' or 'snips' to use dataset provided by us or enter what ever you named your own dataset.\n",
        "                Note, if you don't want to use this part, enter --dataset=''. It can not be None\"\"\")\n",
        "parser.add_argument(\"--model_path\", type=str, default='./src/model', help=\"Path to save model.\")\n",
        "parser.add_argument(\"--vocab_path\", type=str, default='./src/vocab', help=\"Path to vocabulary files.\")\n",
        "\n",
        "#Data\n",
        "parser.add_argument(\"--train_data_path\", type=str, default='train', help=\"Path to training data files.\")\n",
        "parser.add_argument(\"--test_data_path\", type=str, default='test', help=\"Path to testing data files.\")\n",
        "parser.add_argument(\"--valid_data_path\", type=str, default='valid', help=\"Path to validation data files.\")\n",
        "parser.add_argument(\"--input_file\", type=str, default='seq.in', help=\"Input file name.\")\n",
        "parser.add_argument(\"--slot_file\", type=str, default='seq.out', help=\"Slot file name.\")\n",
        "parser.add_argument(\"--intent_file\", type=str, default='label', help=\"Intent file name.\")\n",
        "\n",
        "arg=parser.parse_args([\"--dataset\",\"snips\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oJw4ZYsyRpJn",
        "outputId": "2960c457-bbd7-4d6d-ad0e-b669065cf200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#Print arguments\n",
        "for k,v in sorted(vars(arg).items()):\n",
        "    print(k,'=',v)\n",
        "print()\n",
        "\n",
        "if arg.model_type == 'full':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = False\n",
        "elif arg.model_type == 'intent_only':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = True\n",
        "else:\n",
        "    print('unknown model type!')\n",
        "    exit(1)\n",
        "\n",
        "#full path to data will be: ./data + dataset + train/test/valid\n",
        "if arg.dataset == None:\n",
        "    print('name of dataset can not be None')\n",
        "    exit(1)\n",
        "elif arg.dataset == 'snips':\n",
        "    print('use snips dataset')\n",
        "elif arg.dataset == 'atis':\n",
        "    print('use atis dataset')\n",
        "else:\n",
        "    print('use own dataset: ',arg.dataset)\n",
        "full_train_path = os.path.join('./src/data',arg.dataset,arg.train_data_path)\n",
        "full_test_path = os.path.join('./src/data',arg.dataset,arg.test_data_path)\n",
        "full_valid_path = os.path.join('./src/data',arg.dataset,arg.valid_data_path)\n",
        "\n",
        "createVocabulary(os.path.join(full_train_path, arg.input_file), os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "createVocabulary(os.path.join(full_train_path, arg.slot_file), os.path.join(arg.vocab_path, 'slot_vocab'), flag='slot')\n",
        "createVocabulary(os.path.join(full_train_path, arg.intent_file), os.path.join(arg.vocab_path, 'intent_vocab'),flag='int')\n",
        "\n",
        "in_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "slot_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'slot_vocab'))\n",
        "intent_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'intent_vocab'))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_size = 48\n",
            "dataset = snips\n",
            "early_stop = True\n",
            "input_file = seq.in\n",
            "intent_file = label\n",
            "layer_size = 64\n",
            "max_epochs = 50\n",
            "model_path = ./src/model\n",
            "model_type = full\n",
            "patience = 20\n",
            "slot_file = seq.out\n",
            "test_data_path = test\n",
            "train_data_path = train\n",
            "valid_data_path = valid\n",
            "vocab_path = ./src/vocab\n",
            "\n",
            "use snips dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "22Bbh5PQWFbc",
        "colab": {}
      },
      "source": [
        "class CustomDropout(Layer):\n",
        "    def __init__(self, rate, **kwargs):\n",
        "        super(CustomDropout, self).__init__(**kwargs)\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            return tf.nn.dropout(inputs, rate=self.rate)\n",
        "        return inputs    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk60DUEfcz1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.W2 = tf.keras.layers.Dense(units,kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "\n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXMRBIajcz1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#both query and values are time distributed\n",
        "class CustomBahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(CustomBahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units,kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        #self.W2 = TimeDistributed(tf.keras.layers.Dense(units,kernel_regularizer = tf.keras.regularizers.l2(0.02)))\n",
        "        self.W2 = Conv1D(units,5,1,'same')\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        #hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        hidden_with_time_axis = query\n",
        "\n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        #context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhv2Iqvzcz1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SlotGate(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(SlotGate, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.W2 = tf.keras.layers.Dense(units, kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        #context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p-OKrKFY3fwX",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, input_size, slot_size, intent_size, layer_size = 64):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.attn_size = 2 * layer_size\n",
        "        self.embedding = Embedding(input_size, layer_size)\n",
        "\n",
        "        self.bilstm = Bidirectional(CuDNNLSTM(2*layer_size, return_sequences=True,\n",
        "                                              return_state=True,kernel_regularizer = tf.keras.regularizers.l2(0.02) ))\n",
        "        self.dropout = CustomDropout(0.3)\n",
        "        self.slot_att = CustomBahdanauAttention(2*layer_size)\n",
        "        self.intent_att = BahdanauAttention(2*layer_size)\n",
        "        self.slot_gate = SlotGate(2*layer_size)\n",
        "        #self.slot_proj = Conv1D(slot_size, 5, 1, 'same',activation='tanh')\n",
        "        self.intent_proj = Dense(intent_size, activation='tanh')\n",
        "        self.slot_proj = TimeDistributed(Dense(slot_size))\n",
        "  \n",
        "    @tf.function  \n",
        "    def call(self, inputs, sequence_length, isTraining=True):\n",
        "        x = self.embedding(inputs)\n",
        "        state_outputs, fw_h, fw_c, bw_h, bw_c = self.bilstm(x) \n",
        "        state_outputs = self.dropout(state_outputs, isTraining)\n",
        "        fwd_h = self.dropout(fw_h, isTraining)\n",
        "        bw_h = self.dropout(bw_h, isTraining)\n",
        "        fwd_c = self.dropout(fw_c, isTraining)\n",
        "        bw_c = self.dropout(bw_c, isTraining)\n",
        "        final_state = tf.keras.layers.concatenate([fw_h, fw_c, bw_h, bw_c], -1)\n",
        "\n",
        "        slot_d, _ = self.slot_att(state_outputs, state_outputs)\n",
        "        intent_d, _ = self.intent_att(final_state, state_outputs)\n",
        "\n",
        "        #intent_out = tf.keras.layers.concatenate([intent_d,final_state], -1)\n",
        "        #intent_out = final_state\n",
        "        intent_out = intent_d\n",
        "\n",
        "        slot_gated,_ = self.slot_gate(intent_out, slot_d)\n",
        "\n",
        "        slot_out = tf.keras.layers.concatenate([slot_gated,state_outputs], -1)\n",
        "\n",
        "        intent = self.intent_proj(intent_out)\n",
        "        slots = self.slot_proj(slot_out)\n",
        "        outputs = [slots, intent]\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d4WTGWQX4AH7",
        "colab": {}
      },
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = MyModel(len(in_vocab['vocab']), len(slot_vocab['vocab']), len(intent_vocab['vocab']), layer_size=arg.layer_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "geNgBiAPXffN",
        "colab": {}
      },
      "source": [
        "def valid(in_path, slot_path, intent_path):\n",
        "    data_processor_valid = DataProcessor(in_path, slot_path, intent_path, in_vocab, slot_vocab, intent_vocab)\n",
        "    pred_intents = []\n",
        "    correct_intents = []\n",
        "    slot_outputs = []\n",
        "    correct_slots = []\n",
        "    input_words = []\n",
        "\n",
        "    #used to gate\n",
        "    #gate_seq = []\n",
        "    while True:\n",
        "        in_data, slot_data, slot_weight, length, intents, in_seq, slot_seq, intent_seq = data_processor_valid.get_batch(arg.batch_size)\n",
        "        #feed_dict = {input_data.name: in_data, sequence_length.name: length}\n",
        "        #ret = sess.run(inference_outputs, feed_dict)\n",
        "        slots, intent = model(in_data, length, isTraining = False)\n",
        "        for i in np.array(intent):\n",
        "            pred_intents.append(np.argmax(i))\n",
        "        for i in intents:\n",
        "            correct_intents.append(i)\n",
        "\n",
        "        pred_slots = slots\n",
        "        for p, t, i, l in zip(pred_slots, slot_data, in_data, length):\n",
        "            p = np.argmax(p, 1)\n",
        "            tmp_pred = []\n",
        "            tmp_correct = []\n",
        "            tmp_input = []\n",
        "            for j in range(l):\n",
        "                tmp_pred.append(slot_vocab['rev'][p[j]])\n",
        "                tmp_correct.append(slot_vocab['rev'][t[j]])\n",
        "                tmp_input.append(in_vocab['rev'][i[j]])\n",
        "\n",
        "            slot_outputs.append(tmp_pred)\n",
        "            correct_slots.append(tmp_correct)\n",
        "            input_words.append(tmp_input)\n",
        "\n",
        "        if data_processor_valid.end == 1:\n",
        "            break\n",
        "\n",
        "    pred_intents = np.array(pred_intents)\n",
        "    correct_intents = np.array(correct_intents)\n",
        "    accuracy = (pred_intents==correct_intents)\n",
        "    semantic_error = accuracy\n",
        "    accuracy = accuracy.astype(float)\n",
        "    accuracy = np.mean(accuracy)*100.0\n",
        "\n",
        "    index = 0\n",
        "    for t, p in zip(correct_slots, slot_outputs):\n",
        "        # Process Semantic Error\n",
        "        if len(t) != len(p):\n",
        "            raise ValueError('Error!!')\n",
        "\n",
        "        for j in range(len(t)):\n",
        "            if p[j] != t[j]:\n",
        "                semantic_error[index] = False\n",
        "                break\n",
        "        index += 1\n",
        "    semantic_error = semantic_error.astype(float)\n",
        "    semantic_error = np.mean(semantic_error)*100.0\n",
        "\n",
        "    f1, precision, recall = computeF1Score(correct_slots, slot_outputs)\n",
        "    print('slot f1: ' + str(f1) + '\\tintent accuracy: ' + str(accuracy) + '\\tsemantic_error: ' + str(semantic_error))\n",
        "    #print('intent accuracy: ' + str(accuracy))\n",
        "    #print('semantic error(intent, slots are all correct): ' + str(semantic_error))\n",
        "\n",
        "    data_processor_valid.close()\n",
        "    return f1,accuracy,semantic_error,pred_intents,correct_intents,slot_outputs,correct_slots,input_words#,gate_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SnOPxP7v4bVr",
        "outputId": "c4c417f6-c905-418f-9bde-d07b8c16e125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        }
      },
      "source": [
        "#training\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(-1), optimizer=opt, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, arg.model_path + \"/\" + arg.dataset + '/tf_ckpts', max_to_keep=3)\n",
        "data_processor = None\n",
        "valid_err = 0\n",
        "no_improve = 0\n",
        "save_path = os.path.join(arg.model_path , str(arg.dataset) + \"/\")\n",
        "for epoch in range(50):\n",
        "    while True:\n",
        "        if data_processor == None:\n",
        "            i_loss = 0\n",
        "            s_loss = 0\n",
        "            batches = 0\n",
        "            data_processor = DataProcessor(os.path.join(full_train_path, arg.input_file), os.path.join(full_train_path, arg.slot_file), os.path.join(full_train_path, arg.intent_file), in_vocab, slot_vocab, intent_vocab)\n",
        "        in_data, slot_labels, slot_weights, length, intent_labels,in_seq,_,_ = data_processor.get_batch(arg.batch_size)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            slots, intent = model(in_data, length, isTraining = True)\n",
        "            intent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=intent_labels, logits=intent)  \n",
        "            intent_loss = tf.reduce_sum(intent_loss)/tf.cast(arg.batch_size, tf.float32)\n",
        "            slots_out = tf.reshape(slots, [-1,len(slot_vocab['vocab'])])\n",
        "            slots_shape = tf.shape(slot_labels)\n",
        "            slot_reshape = tf.reshape(slot_labels, [-1])\n",
        "            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=slot_reshape, logits=slots_out)\n",
        "            crossent = tf.reshape(crossent, slots_shape)\n",
        "            slot_loss = tf.reduce_sum(crossent*slot_weights, 1)\n",
        "            total_size = tf.reduce_sum(slot_weights, 1)\n",
        "            total_size += 1e-12\n",
        "            slot_loss = slot_loss/ total_size\n",
        "            slot_loss = tf.reduce_sum(slot_loss)\n",
        "            \n",
        "            total_loss = intent_loss + slot_loss + (tf.reduce_sum(model.losses)/50)    #intent_loss + \n",
        "        \n",
        "        grads = tape.gradient(total_loss, model.trainable_weights)\n",
        "        opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        s_loss = s_loss + tf.reduce_sum(slot_loss)/tf.cast(arg.batch_size, tf.float32)\n",
        "        i_loss = i_loss +  tf.reduce_sum(intent_loss)/tf.cast(arg.batch_size, tf.float32)\n",
        "        batches = batches + 1\n",
        "        if data_processor.end == 1:\n",
        "            data_processor.close()\n",
        "            data_processor = None\n",
        "            break\n",
        "            \n",
        "    #print(\"Training Epoch: \" ,epoch,\" Slot Loss: \",s_loss/batches, \" Intent_Loss: \", i_loss/batches)\n",
        "    print(\"EPOCH: \", epoch, \" *******************************************************************\")\n",
        "    print('Train:', end=\"\\t\")\n",
        "    _ = valid(os.path.join(full_train_path, arg.input_file), os.path.join(full_train_path, arg.slot_file), os.path.join(full_train_path, arg.intent_file))\n",
        "    \n",
        "    print('Valid:', end=\"\\t\")\n",
        "    epoch_valid_slot, epoch_valid_intent, epoch_valid_err,valid_pred_intent,valid_correct_intent,valid_pred_slot,valid_correct_slot,valid_words = valid(os.path.join(full_valid_path, arg.input_file), os.path.join(full_valid_path, arg.slot_file), os.path.join(full_valid_path, arg.intent_file))\n",
        "\n",
        "    print('Test:', end=\"\\t\")\n",
        "    epoch_test_slot, epoch_test_intent, epoch_test_err,test_pred_intent,test_correct_intent,test_pred_slot,test_correct_slot,test_words = valid(os.path.join(full_test_path, arg.input_file), os.path.join(full_test_path, arg.slot_file), os.path.join(full_test_path, arg.intent_file))\n",
        "    \n",
        "    ckpt.step.assign_add(1)\n",
        "    if epoch_valid_err <= valid_err:\n",
        "        no_improve += 1\n",
        "    else:\n",
        "        valid_err = epoch_valid_err\n",
        "        no_improve = 0\n",
        "        print(\"Saving\", str(ckpt.step), \"with valid accuracy:\", valid_err   )\n",
        "        save_path = manager.save()\n",
        "\n",
        "    if arg.early_stop == True:\n",
        "        if no_improve > arg.patience:\n",
        "            print(\"EARLY BREAK\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH:  0  *******************************************************************\n",
            "Train:\tslot f1: 49.30110087117286\tintent accuracy: 70.9033934576582\tsemantic_error: 17.945582390706207\n",
            "Valid:\tslot f1: 47.49003984063745\tintent accuracy: 68.57142857142857\tsemantic_error: 15.714285714285714\n",
            "Test:\tslot f1: 46.27181385510312\tintent accuracy: 71.71428571428572\tsemantic_error: 17.57142857142857\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> with valid accuracy: 15.714285714285714\n",
            "EPOCH:  1  *******************************************************************\n",
            "Train:\tslot f1: 68.51097770874439\tintent accuracy: 82.08498929990829\tsemantic_error: 40.47691837358606\n",
            "Valid:\tslot f1: 64.50250725785166\tintent accuracy: 80.0\tsemantic_error: 34.85714285714286\n",
            "Test:\tslot f1: 63.80498145204028\tintent accuracy: 81.85714285714286\tsemantic_error: 35.85714285714286\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1> with valid accuracy: 34.85714285714286\n",
            "EPOCH:  2  *******************************************************************\n",
            "Train:\tslot f1: 79.03450234275168\tintent accuracy: 83.17792723937633\tsemantic_error: 53.3934576582085\n",
            "Valid:\tslot f1: 72.9693741677763\tintent accuracy: 80.28571428571428\tsemantic_error: 44.42857142857143\n",
            "Test:\tslot f1: 71.32978723404256\tintent accuracy: 80.85714285714286\tsemantic_error: 44.0\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2> with valid accuracy: 44.42857142857143\n",
            "EPOCH:  3  *******************************************************************\n",
            "Train:\tslot f1: 86.17040971816746\tintent accuracy: 96.09446652399878\tsemantic_error: 69.66523998777132\n",
            "Valid:\tslot f1: 78.75565009306034\tintent accuracy: 94.28571428571428\tsemantic_error: 56.14285714285714\n",
            "Test:\tslot f1: 76.55755015839492\tintent accuracy: 92.85714285714286\tsemantic_error: 54.0\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3> with valid accuracy: 56.14285714285714\n",
            "EPOCH:  4  *******************************************************************\n",
            "Train:\tslot f1: 90.24032234853937\tintent accuracy: 98.1809844084378\tsemantic_error: 77.9883827575665\n",
            "Valid:\tslot f1: 81.68938786420743\tintent accuracy: 95.85714285714285\tsemantic_error: 60.857142857142854\n",
            "Test:\tslot f1: 79.76601967561818\tintent accuracy: 94.42857142857143\tsemantic_error: 60.71428571428571\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=4> with valid accuracy: 60.857142857142854\n",
            "EPOCH:  5  *******************************************************************\n",
            "Train:\tslot f1: 92.88665323340533\tintent accuracy: 98.53255885050444\tsemantic_error: 83.37664322837053\n",
            "Valid:\tslot f1: 82.70313757039422\tintent accuracy: 95.57142857142857\tsemantic_error: 63.142857142857146\n",
            "Test:\tslot f1: 81.52900294039027\tintent accuracy: 95.42857142857143\tsemantic_error: 63.714285714285715\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5> with valid accuracy: 63.142857142857146\n",
            "EPOCH:  6  *******************************************************************\n",
            "Train:\tslot f1: 94.2223514869483\tintent accuracy: 98.73127483949862\tsemantic_error: 86.26566799143993\n",
            "Valid:\tslot f1: 84.14798811774237\tintent accuracy: 96.14285714285714\tsemantic_error: 65.57142857142857\n",
            "Test:\tslot f1: 82.41935483870968\tintent accuracy: 95.14285714285714\tsemantic_error: 64.0\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=6> with valid accuracy: 65.57142857142857\n",
            "EPOCH:  7  *******************************************************************\n",
            "Train:\tslot f1: 94.16804623740234\tintent accuracy: 95.98746560684805\tsemantic_error: 84.69886884744726\n",
            "Valid:\tslot f1: 84.0909090909091\tintent accuracy: 95.42857142857143\tsemantic_error: 64.85714285714286\n",
            "Test:\tslot f1: 81.92318023099651\tintent accuracy: 90.71428571428571\tsemantic_error: 61.0\n",
            "EPOCH:  8  *******************************************************************\n",
            "Train:\tslot f1: 95.83351600976223\tintent accuracy: 98.12748394986242\tsemantic_error: 89.04769183735861\n",
            "Valid:\tslot f1: 84.23631908521645\tintent accuracy: 95.57142857142857\tsemantic_error: 65.57142857142857\n",
            "Test:\tslot f1: 83.00352399024126\tintent accuracy: 94.57142857142857\tsemantic_error: 64.85714285714286\n",
            "EPOCH:  9  *******************************************************************\n",
            "Train:\tslot f1: 96.4421557727193\tintent accuracy: 98.57841638642617\tsemantic_error: 90.88963619688168\n",
            "Valid:\tslot f1: 84.7725415418142\tintent accuracy: 95.28571428571428\tsemantic_error: 66.0\n",
            "Test:\tslot f1: 83.14424635332253\tintent accuracy: 94.71428571428572\tsemantic_error: 65.14285714285715\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=9> with valid accuracy: 66.0\n",
            "EPOCH:  10  *******************************************************************\n",
            "Train:\tslot f1: 97.27589948098408\tintent accuracy: 98.71598899419138\tsemantic_error: 92.47936410883521\n",
            "Valid:\tslot f1: 84.40567066521265\tintent accuracy: 95.85714285714285\tsemantic_error: 65.28571428571428\n",
            "Test:\tslot f1: 83.35135135135135\tintent accuracy: 93.85714285714286\tsemantic_error: 65.28571428571428\n",
            "EPOCH:  11  *******************************************************************\n",
            "Train:\tslot f1: 97.79121475850125\tintent accuracy: 99.02170590033629\tsemantic_error: 93.82451849587282\n",
            "Valid:\tslot f1: 84.51472191930208\tintent accuracy: 96.0\tsemantic_error: 66.57142857142857\n",
            "Test:\tslot f1: 83.50040639393117\tintent accuracy: 95.57142857142857\tsemantic_error: 66.85714285714286\n",
            "Saving <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=11> with valid accuracy: 66.57142857142857\n",
            "EPOCH:  12  *******************************************************************\n",
            "Train:\tslot f1: 97.12286044775028\tintent accuracy: 97.82176704371753\tsemantic_error: 91.89085906450627\n",
            "Valid:\tslot f1: 85.08317425688573\tintent accuracy: 94.71428571428572\tsemantic_error: 66.57142857142857\n",
            "Test:\tslot f1: 82.5499729875743\tintent accuracy: 95.14285714285714\tsemantic_error: 63.857142857142854\n",
            "EPOCH:  13  *******************************************************************\n",
            "Train:\tslot f1: 98.28338154745296\tintent accuracy: 99.15163558544788\tsemantic_error: 95.13145826964231\n",
            "Valid:\tslot f1: 84.67193030220528\tintent accuracy: 95.42857142857143\tsemantic_error: 64.85714285714286\n",
            "Test:\tslot f1: 83.65800865800867\tintent accuracy: 95.85714285714285\tsemantic_error: 66.14285714285715\n",
            "EPOCH:  14  *******************************************************************\n",
            "Train:\tslot f1: 98.61141723605907\tintent accuracy: 99.18985019871599\tsemantic_error: 95.92632222561907\n",
            "Valid:\tslot f1: 85.3238589778628\tintent accuracy: 97.28571428571429\tsemantic_error: 66.42857142857143\n",
            "Test:\tslot f1: 83.53609083536091\tintent accuracy: 96.0\tsemantic_error: 66.0\n",
            "EPOCH:  15  *******************************************************************\n",
            "Train:\tslot f1: 98.74961430523517\tintent accuracy: 99.20513604402323\tsemantic_error: 96.26261082237848\n",
            "Valid:\tslot f1: 84.76712328767123\tintent accuracy: 96.0\tsemantic_error: 65.42857142857143\n",
            "Test:\tslot f1: 84.05875952121872\tintent accuracy: 96.57142857142857\tsemantic_error: 67.14285714285714\n",
            "EPOCH:  16  *******************************************************************\n",
            "Train:\tslot f1: 99.01496662648124\tintent accuracy: 99.44970956893916\tsemantic_error: 96.96575970651176\n",
            "Valid:\tslot f1: 84.79212253829321\tintent accuracy: 96.85714285714285\tsemantic_error: 66.14285714285715\n",
            "Test:\tslot f1: 85.10754151919413\tintent accuracy: 95.85714285714285\tsemantic_error: 67.85714285714286\n",
            "EPOCH:  17  *******************************************************************\n",
            "Train:\tslot f1: 99.01065784638\tintent accuracy: 99.35035157444206\tsemantic_error: 96.98868847447264\n",
            "Valid:\tslot f1: 84.25648021828104\tintent accuracy: 95.42857142857143\tsemantic_error: 64.71428571428571\n",
            "Test:\tslot f1: 83.5723951285521\tintent accuracy: 96.57142857142857\tsemantic_error: 66.71428571428571\n",
            "EPOCH:  18  *******************************************************************\n",
            "Train:\tslot f1: 98.94786339857757\tintent accuracy: 98.01284011005808\tsemantic_error: 95.95689391623357\n",
            "Valid:\tslot f1: 83.77789874795863\tintent accuracy: 93.57142857142857\tsemantic_error: 62.71428571428571\n",
            "Test:\tslot f1: 83.70832203849281\tintent accuracy: 94.28571428571428\tsemantic_error: 67.0\n",
            "EPOCH:  19  *******************************************************************\n",
            "Train:\tslot f1: 99.19105750845712\tintent accuracy: 99.43442372363191\tsemantic_error: 97.4778355243045\n",
            "Valid:\tslot f1: 83.46542771248974\tintent accuracy: 95.85714285714285\tsemantic_error: 63.142857142857146\n",
            "Test:\tslot f1: 83.6413043478261\tintent accuracy: 96.28571428571429\tsemantic_error: 65.28571428571428\n",
            "EPOCH:  20  *******************************************************************\n",
            "Train:\tslot f1: 99.31610606974249\tintent accuracy: 99.2968511158667\tsemantic_error: 97.65362274533783\n",
            "Valid:\tslot f1: 84.5039628313747\tintent accuracy: 96.28571428571429\tsemantic_error: 64.14285714285714\n",
            "Test:\tslot f1: 83.89553862894451\tintent accuracy: 96.0\tsemantic_error: 66.85714285714286\n",
            "EPOCH:  21  *******************************************************************\n",
            "Train:\tslot f1: 99.41740716765729\tintent accuracy: 99.62549678997249\tsemantic_error: 98.09691225924794\n",
            "Valid:\tslot f1: 84.59858001092299\tintent accuracy: 96.0\tsemantic_error: 65.14285714285715\n",
            "Test:\tslot f1: 84.23631908521645\tintent accuracy: 96.0\tsemantic_error: 66.57142857142857\n",
            "EPOCH:  22  *******************************************************************\n",
            "Train:\tslot f1: 99.4275960505599\tintent accuracy: 99.52613879547539\tsemantic_error: 98.07398349128707\n",
            "Valid:\tslot f1: 84.50164293537787\tintent accuracy: 96.57142857142857\tsemantic_error: 64.42857142857143\n",
            "Test:\tslot f1: 84.20479302832246\tintent accuracy: 96.42857142857143\tsemantic_error: 67.42857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N5ElRUZNBq8",
        "colab_type": "code",
        "outputId": "8daecc73-67ed-499c-8944-a0a544292ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        " model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  719552    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional multiple                  66560     \n",
            "_________________________________________________________________\n",
            "custom_dropout (CustomDropou multiple                  0         \n",
            "_________________________________________________________________\n",
            "custom_bahdanau_attention (C multiple                  98689     \n",
            "_________________________________________________________________\n",
            "bahdanau_attention (Bahdanau multiple                  49537     \n",
            "_________________________________________________________________\n",
            "slot_gate (SlotGate)         multiple                  33153     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              multiple                  2695      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri multiple                  18761     \n",
            "=================================================================\n",
            "Total params: 988,947\n",
            "Trainable params: 988,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4o15eK9npnOS",
        "outputId": "88327144-6dca-4303-f13a-b59bdca56d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        }
      },
      "source": [
        "#Let's try to clear slate and reload maodel  .....\n",
        "import time\n",
        "tf.keras.backend.clear_session()\n",
        "if arg.dataset == 'atis':\n",
        "    test_batch = 893\n",
        "elif arg.dataset == 'snips':\n",
        "    test_batch = 700\n",
        "\n",
        "model = MyModel(len(in_vocab['vocab']), len(slot_vocab['vocab']), len(intent_vocab['vocab']), layer_size=arg.layer_size)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(-1), optimizer=opt, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, arg.model_path + \"/\" + arg.dataset + '/tf_ckpts', max_to_keep=3)\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "\n",
        "data_processor_test = DataProcessor(os.path.join(full_test_path, arg.input_file), os.path.join(full_test_path, arg.slot_file), os.path.join(full_test_path, arg.intent_file), in_vocab, slot_vocab, intent_vocab)\n",
        "in_data, slot_labels, slot_weights, length, intent_labels,in_seq,_,_ = data_processor_test.get_batch(test_batch)\n",
        "data_processor_test.close()\n",
        "\n",
        "\n",
        "\n",
        "t = time.perf_counter()\n",
        "slots, intent = model(in_data, length, isTraining = False)\n",
        "elapsed = time.perf_counter() - t\n",
        "print(\"Milli seconds per query:\", (elapsed*1000)/float(100.0))\n",
        "\n",
        "pred_intents = []\n",
        "correct_intents = []\n",
        "slot_outputs = []\n",
        "correct_slots = []\n",
        "input_words = []\n",
        "\n",
        "for i in np.array(intent):\n",
        "    pred_intents.append(np.argmax(i))\n",
        "for i in intent_labels:\n",
        "    correct_intents.append(i)\n",
        "\n",
        "pred_slots = slots\n",
        "for p, t, i, l in zip(pred_slots, slot_labels, in_data, length):\n",
        "    p = np.argmax(p, 1)\n",
        "    tmp_pred = []\n",
        "    tmp_correct = []\n",
        "    tmp_input = []\n",
        "    for j in range(l):\n",
        "        tmp_pred.append(slot_vocab['rev'][p[j]])\n",
        "        tmp_correct.append(slot_vocab['rev'][t[j]])\n",
        "        tmp_input.append(in_vocab['rev'][i[j]])\n",
        "\n",
        "    slot_outputs.append(tmp_pred)\n",
        "    correct_slots.append(tmp_correct)\n",
        "    input_words.append(tmp_input)\n",
        "\n",
        "pred_intents = np.array(pred_intents)\n",
        "correct_intents = np.array(correct_intents)\n",
        "accuracy = (pred_intents==correct_intents)\n",
        "semantic_error = accuracy\n",
        "accuracy = accuracy.astype(float)\n",
        "accuracy = np.mean(accuracy)*100.0\n",
        "\n",
        "index = 0\n",
        "for t, p in zip(correct_slots, slot_outputs):\n",
        "    # Process Semantic Error\n",
        "    if len(t) != len(p):\n",
        "        raise ValueError('Error!!')\n",
        "\n",
        "    for j in range(len(t)):\n",
        "        if p[j] != t[j]:\n",
        "            semantic_error[index] = False\n",
        "            break\n",
        "    index += 1\n",
        "semantic_error = semantic_error.astype(float)\n",
        "semantic_error = np.mean(semantic_error)*100.0\n",
        "\n",
        "f1, precision, recall = computeF1Score(correct_slots, slot_outputs)\n",
        "print('slot f1: ' + str(f1) + '\\tintent accuracy: ' + str(accuracy) + '\\tsemantic_accuracy: ' + str(semantic_error))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0609 19:14:56.006635 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca76d6d8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.012698 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca7a48b8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.020207 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca7772c8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.024991 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca357818> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.030335 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca79d868> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.036199 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca765a48> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.041149 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca357908> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.046097 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaae64bd18> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.051466 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca740c28> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.056752 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca740c78> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.061913 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faa416243b8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.067067 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaae64bcc8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.072096 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faa41624458> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.077557 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca77bb88> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.082532 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca71a368> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.089124 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca357bd8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.094215 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca6a0958> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.099104 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca7a44a8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.103554 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faa41624638> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.108500 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca357188> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.114460 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaaf4390e8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.119801 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca775d68> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.124141 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca630688> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.129992 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca3572c8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.135267 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faa41624818> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.140392 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca357318> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.144713 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faa41624098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.150649 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faa415fca98> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "W0609 19:14:56.155470 140374735677312 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7faaca357368> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca76d6d8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca7a48b8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca7772c8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca357818> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca79d868> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca765a48> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca357908> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaae64bd18> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca740c28> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca740c78> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faa416243b8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaae64bcc8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faa41624458> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca77bb88> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca71a368> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca357bd8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca6a0958> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca7a44a8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faa41624638> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca357188> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaaf4390e8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca775d68> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca630688> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca3572c8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faa41624818> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca357318> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faa41624098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faa415fca98> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x7faaca357368> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
            "Milli seconds per query: 33.75181055000212\n",
            "slot f1: 83.16616969368391\tintent accuracy: 70.28571428571428\tsemantic_accuracy: 45.857142857142854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "59zw1hriEAUL",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}